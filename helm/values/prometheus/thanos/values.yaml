nameOverride: "prometheus"
fullnameOverride: "prometheus"
## Create default rules for monitoring the cluster
##
defaultRules:
  create: true
  rules:
    alertmanager: true
    etcd: true
    configReloaders: true
    general: true
    k8sContainerCpuUsageSecondsTotal: true
    k8sContainerMemoryCache: true
    k8sContainerMemoryRss: true
    k8sContainerMemorySwap: true
    k8sContainerResource: true
    k8sContainerMemoryWorkingSetBytes: true
    k8sPodOwner: true
    kubeApiserverAvailability: true
    kubeApiserverBurnrate: true
    kubeApiserverHistogram: true
    kubeApiserverSlos: true
    kubeControllerManager: true
    kubelet: true
    kubeProxy: true
    kubePrometheusGeneral: true
    kubePrometheusNodeRecording: true
    kubernetesApps: true
    kubernetesResources: true
    kubernetesStorage: true
    kubernetesSystem: true
    kubeSchedulerAlerting: true
    kubeSchedulerRecording: true
    kubeStateMetrics: true
    network: true
    node: true
    nodeExporterAlerting: true
    nodeExporterRecording: true
    prometheus: true
    prometheusOperator: true
    windows: false

  ## Disabled PrometheusRule alerts
  disabled: {}
  # KubeAPIDown: true
  # NodeRAIDDegraded: true

windowsMonitoring:
  ## Deploys the windows-exporter and Windows-specific dashboards and rules (job name must be 'windows-exporter')
  enabled: false

## Configuration for prometheus-windows-exporter
## ref: https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-windows-exporter
##
prometheus-windows-exporter:
  ## Enable ServiceMonitor and set Kubernetes label to use as a job label
  ##
  prometheus:
    monitor:
      enabled: true
      jobLabel: jobLabel

  releaseLabel: true

  ## Set job label to 'windows-exporter' as required by the default Prometheus rules and Grafana dashboards
  ##
  podLabels:
    jobLabel: windows-exporter

  ## Enable memory and container metrics as required by the default Prometheus rules and Grafana dashboards
  ##
  config: |-
    collectors:
      enabled: '[defaults],memory,container'

## Component scraping etcd
##
kubeEtcd:
  enabled: true

## ref: https://prometheus.io/docs/alerting/alertmanager/
##
alertmanager:
  ## Deploy alertmanager
  ##
  enabled: true
  ## Alertmanager configuration directives
  ## ref: https://prometheus.io/docs/alerting/configuration/#configuration-file
  ##      https://prometheus.io/webtools/alerting/routing-tree-editor/
  ##
  config:
    global:
      resolve_timeout: 5m
    route:
      group_wait: 30s
      group_interval: 1m
      repeat_interval: 12h
      receiver: slack_others
      routes:
      - receiver: slack_critical
        # continue: true
        match:
          severity: critical
      - receiver: slack_warning
        # continue: true
        match:
          severity: warning
      # - receiver: email_alerts
      #   match_re:
      #     severity: critical|warning
    receivers:
    - name: slack_others
      slack_configs:
      - api_url: "https://hooks.slack.com/services/TB5FXBSUE/B041XD27KHV/A7Z4C8jUdEJhcqvxOvjiMC"
        send_resolved: true
        icon_url: https://avatars3.githubusercontent.com/u/3380462
        title: |-
          [{{ .Status | toUpper }}{{ if eq .Status "firing" }}: {{ .Alerts.Firing | len }}{{ end }}]
        text: |-
          {{ range .Alerts }}

          *Alert Name :* {{ .Labels.alertname }}

          {{- if .Annotations.summary }}
          *Alert Summary:* {{ .Annotations.summary }}
          {{- end -}}

          {{- if .Annotations.description }}
          *Alert Description:* {{ .Annotations.description }}
          {{ else }}
          *Alert Message:* {{ .Annotations.message }}
          {{- end }}

          *Alert Details:*
          {{ range .Labels.SortedPairs }} • *{{ .Name }}:* `{{ .Value }}`
          {{ end }}
          {{ end }}

    - name: slack_critical
      slack_configs:
      - api_url: "https://hooks.slack.com/services/TB5FXBSUE/B041XD27KHV/WcA7Z8jUdEJhcqvxOvjiMC"
        send_resolved: true
        icon_url: https://avatars3.githubusercontent.com/u/3380462
        title: |-
          [{{ .Status | toUpper }}{{ if eq .Status "firing" }}: {{ .Alerts.Firing | len }}{{ end }}]
        text: |-
          {{ range .Alerts }}

          *Alert Name :* {{ .Labels.alertname }}

          {{- if .Annotations.summary }}
          *Alert Summary:* {{ .Annotations.summary }}
          {{- end -}}

          {{- if .Annotations.description }}
          *Alert Description:* {{ .Annotations.description }}
          {{ else }}
          *Alert Message:* {{ .Annotations.message }}
          {{- end }}

          *Alert Details:*
          {{ range .Labels.SortedPairs }} • *{{ .Name }}:* `{{ .Value }}`
          {{ end }}
          {{ end }}

    - name: slack_warning
      slack_configs:
      - api_url: "https://hooks.slack.com/services/TB5FXBSUE/B041XD27KHV/WcA7Z4C8jEJhcqvxOvjiMC"
        send_resolved: true
        icon_url: https://avatars3.githubusercontent.com/u/3380462
        title: |-
          [{{ .Status | toUpper }}{{ if eq .Status "firing" }}: {{ .Alerts.Firing | len }}{{ end }}]
        text: |-
          {{ range .Alerts }}

          *Alert Name :* {{ .Labels.alertname }}

          {{- if .Annotations.summary }}
          *Alert Summary:* {{ .Annotations.summary }}
          {{- end -}}

          {{- if .Annotations.description }}
          *Alert Description:* {{ .Annotations.description }}
          {{ else }}
          *Alert Message:* {{ .Annotations.message }}
          {{- end }}

          *Alert Details:*
          {{ range .Labels.SortedPairs }} • *{{ .Name }}:* `{{ .Value }}`
          {{ end }}
          {{ end }}


  alertmanagerSpec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: "Addons-Services"
              operator: In
              values:
              - "true"

grafana:
  enabled: ${grafana_enabled}
  serviceAccount:
    annotations: ${annotations}
  priorityClassName: grafana-pod-critical
  defaultDashboardsTimezone: browser
  replicas: 1
  autoscaling:
    enabled: ${grafana_ha_enabled}
    minReplicas: 1
    maxReplicas: 5
    metrics:
    - type: Resource
      resource:
        name: cpu
        targetAverageUtilization: 85
    - type: Resource
      resource:
        name: memory
        targetAverageUtilization: 85

  sidecar:
    dashboards:
      enabled: true
      label: grafana_dashboard
      labelValue: "1"
      # Allow discovery in all namespaces for dashboards
      searchNamespace: ALL
      annotations: ${annotations}
      # Support for new table panels, when enabled grafana auto migrates the old table panels to newer table panels
      enableNewTablePanelSyntax: true
      folder: /tmp/dashboards
      folderAnnotation: grafana_folder
      provider:
        allowUiUpdates: true
        foldersFromFilesStructure: true

    resources:
      limits:
        cpu: 200m
        memory: 400Mi
      requests:
        cpu: 50m
        memory: 100Mi

    datasources:
      enabled: true
      defaultDatasourceEnabled: true
      isDefaultDatasource: false
    # datasources.yaml:
    #   apiVersion: 1
    #   datasources:
    #   - name: prometheus
    #     type: prometheus
    #     url: http://prometheus-operator-kube-p-prometheus.monitoring.svc.cluster.local:9090
    #     access: proxy
    #   - name: loki
    #     type: loki
    #     url: http://loki:3100
    #     access: proxy

  additionalDataSources:
   ${indent(3, loki_datasource_config)}
   ${indent(3, cw_datasource_config)}
   ${indent(3, tempo_datasource_config)}
   ${indent(3, thanos_datasource_config)}

  persistence:
    enabled: true
    storageClassName: ${storage_class_name}
    size: 20Gi

  adminPassword: ${grafana_admin_password}

  ingress:
    enabled: ${ingress_enabled}
    ingressClassName: ${ingress_ingressClassName}
    annotations: ${ingress_annotations}
    hosts: ${ingress_hosts}
    tls: ${ingress_tls}

  serviceMonitor:
    enabled: true
    labels:
      release: prometheus-operator

  grafana.ini:
    max_idle_connections: 500
    dashboards:
      min_refresh_interval: ${min_refresh_interval}
    server:
      enable_gzip: true

  resources:
    limits:
      cpu: 1000m
      memory: 3Gi
    requests:
      cpu: 200m
      memory: 400Mi

  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: app.kubernetes.io/name
            operator: In
            values:
            - grafana
        topologyKey: topology.kubernetes.io/zone
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: "Addons-Services"
            operator: In
            values:
            - "true"

kube-state-metrics:
  metricLabelsAllowlist:
  - pods=[*]
  - nodes=[*]
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: "Addons-Services"
            operator: In
            values:
            - "true"

kubeProxy:
  enabled: false

kubeApiServer:
  enabled: false

kubeControllerManager:
  enabled: false

kubeScheduler:
  enabled: false

prometheusOperator:
  createCustomResource: false
  enabled: true

  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: "Addons-Services"
            operator: In
            values:
            - "true"

nodeExporter:
  enabled: true
  resources:
    limits:
      cpu: 200m
      memory: 600Mi
    requests:
      cpu: 50m
      memory: 100Mi

prometheus:
  enabled: true
  prometheusSpec:
    priorityClassName: system-node-critical
    enableRemoteWriteReceiver: true
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: ${storage_class_name}
          resources:
            requests:
              storage: 30Gi
    retention: 14d
    walCompression: true
    ## If true, the Operator won't process any Prometheus configuration changes
    ##
    paused: false
    ## Number of replicas of each shard to deploy for a Prometheus deployment.
    ## Number of replicas multiplied by shards is the total number of Pods created.
    ##
    replicas: ${prometheus_replicas}
    ## EXPERIMENTAL: Number of shards to distribute targets onto.
    ## Number of replicas multiplied by shards is the total number of Pods created.
    ## Note that scaling down shards will not reshard data onto remaining instances, it must be manually moved.
    ## Increasing shards will not reshard data either but it will continue to be available from the same instances.
    ## To query globally use Thanos sidecar and Thanos querier or remote write data to a central location.
    ## Sharding is done on the content of the `__address__` target meta-label.
    ##
    shards: ${prometheus_shards}
    resources:
      limits:
        cpu: 1200m
        memory: 4Gi
      requests:
        cpu: 200m
        memory: 800Mi

    # remoteWrite:
    #   - url: http://grafana-mimir-nginx.monitoring.svc:80/api/v1/push

    thanos:
      objectStorageConfig:
        secret:
          type: S3
          config:
            bucket: ${s3_bucket_name}
            endpoint: ${s3_endpoint}
            region: ${s3_bucket_region}
            # access_key: ""
            # secret_key: ""

    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: "Addons-Services"
              operator: In
              values:
              - "true"
    additionalScrapeConfigs: []
      # - job_name: blackbox
      #   metrics_path: /probe
      #   params:
      #     module: [http_2xx]
      #   static_configs:
      #     # Add URLs as target parameter
      #     - targets:
      #       - https://www.google.com
      #       - https://stackoverflow.com
      #       - https://scala-lang.org
      #       - https://helm.sh

      #   relabel_configs:
      #   - source_labels: [__address__]
      #     target_label: __param_target
      #   - source_labels: [__param_target]
      #     # Important!
      #     target_label: target
      #     # Ensure blackbox-exporter is reachable from Prometheus
      #   - target_label: __address__
      #     replacement: blackbox-exporter-prometheus-blackbox-exporter:9115
    maximumStartupDurationSeconds: 900
  ingress:
    enabled: ${enable_prometheus_internal_ingress}
    annotations:
      kubernetes.io/ingress.class: "internal-nginx"
      kubernetes.io/tls-acme: "false"
    hosts:
      - ${prometheus_hostname}
    paths:
      - /

prometheus-node-exporter:
  resources:
    limits:
      cpu: 200m
      memory: 600Mi
    requests:
      cpu: 10m
      memory: 50Mi

additionalPrometheusRulesMap:
  nodes:
    groups:
      - name: ethtool
        rules:
        - alert: conntrack_allowance_available
          annotations:
            description: conntrack_allowance_available is the number of tracked connections that can be established by the instance before hitting the Connections Tracked allowance of that instance type `{{$labels.instance }}`
            summary: conntrack_allowance_available `{{ $labels.instance }}`
          expr: node_net_ethtool{type="conntrack_allowance_available"} > 3000
          for: 30s
          labels:
            severity: critical
        - alert: conntrack_allowance_exceeded
          annotations:
            description: conntrack_allowance_exceeded is the number of packets dropped because connection tracking exceeded the maximum for the instance and new connections could not be established `{{$labels.instance }}`
            summary: conntrack_allowance_exceeded `{{ $labels.instance }}`
          expr: node_net_ethtool{type="conntrack_allowance_exceeded"} > 0
          for: 30s
          labels:
            severity: critical
        - alert: pps_allowance_exceeded
          annotations:
            description: pps_allowance_exceeded is the number of packets queued and/or dropped because the bidirectional PPS exceeded the maximum for the instance `{{$labels.instance }}`
            summary: pps_allowance_exceeded `{{ $labels.instance }}`
          expr: node_net_ethtool{type="pps_allowance_exceeded"} > 0
          for: 30s
          labels:
            severity: critical
        - alert: bw_in_allowance_exceeded
          annotations:
            description: bw_in_allowance_exceeded is the number of packets queued and/or dropped because the inbound aggregate bandwidth exceeded the maximum for the instance `{{$labels.instance }}`
            summary: bw_in_allowance_exceeded `{{ $labels.instance }}`
          expr: node_net_ethtool{type="bw_in_allowance_exceeded"} > 0
          for: 30s
          labels:
            severity: critical
        - alert: bw_out_allowance_exceeded
          annotations:
            description: bw_out_allowance_exceeded is the number of packets queued and/or dropped because the outbound aggregate bandwidth exceeded the maximum for the instance `{{$labels.instance }}`
            summary: bw_out_allowance_exceeded `{{ $labels.instance }}`
          expr: node_net_ethtool{type="bw_out_allowance_exceeded"} > 0
          for: 30s
          labels:
            severity: critical
      - name: blackbox
        rules:
        - alert: BlackboxProbeFailed
          expr: probe_success == 0
          for: 30s
          labels:
            severity: critical
          annotations:
            summary: Blackbox probe failed `{{ $labels.instance }}`
            description: Probe failed on `{{ $labels.instance }}`

        - alert: BlackboxProbeHttpFailure
          expr: probe_http_status_code <= 199 OR probe_http_status_code >= 400
          for: 30s
          labels:
            severity: critical
          annotations:
            summary: Blackbox probe HTTP failure `{{ $labels.instance }}`
            description: HTTP status code is not in between 200-399 on `{{ $labels.instance }}`

        - alert: BlackboxSlowProbe
          expr: avg_over_time(probe_duration_seconds[1m]) > 30
          for: 1m
          labels:
            severity: warning
          annotations:
            summary: Blackbox slow probe `{{ $labels.instance }}`
            description: Blackbox probe took more than 30 seconds to complete on `{{ $labels.instance }}`

        - alert: BlackboxProbeSlowHttp
          expr: avg_over_time(probe_http_duration_seconds[1m]) > 30
          for: 1m
          labels:
            severity: warning
          annotations:
            summary: Blackbox probe slow HTTP `{{ $labels.instance }}`
            description: HTTP request took more than 30 seconds to complete on `{{ $labels.instance }}`

        - alert: BlackboxProbeSlowPing
          expr: avg_over_time(probe_icmp_duration_seconds[1m]) > 30
          for: 1m
          labels:
            severity: warning
          annotations:
            summary: Blackbox probe slow ping `{{ $labels.instance }}`
            description: Blackbox ping took more than 30 seconds to complete on `{{ $labels.instance }}`

        - alert: BlackboxSslCertificateWillExpireSoon
          expr: probe_ssl_earliest_cert_expiry - time() < 86400 * 30
          for: 1m
          labels:
            severity: warning
          annotations:
            summary: Blackbox SSL certificate will expire soon `{{ $labels.instance }}`
            description: SSL certificate expires in 30 days on `{{ $labels.instance }}`
      - name: mysql
        rules:
        - alert: MysqlDown
          expr: mysql_up == 0
          for: 1s
          labels:
            severity: critical
          annotations:
            summary: MySQL down (instance {{ $labels.instance }})
            description: MySQL instance is down on {{ $labels.instance }}\n  VALUE = {{ $value }}\n  LABELS= {{ $labels }}
        - alert: MysqlTooManyConnections(>80%)
          expr: avg by (instance) (mysql_global_status_threads_connected) / avg by (instance) (mysql_global_variables_max_connections) * 100 > 80
          for: 2m
          labels:
            severity: warning
          annotations:
            summary: MySQL too many connections (> 80%) (instance {{ $labels.instance }})
            description: More than 80% of MySQL connections are in use on {{ $labels.instance }}\n  VALUE = {{ $value }}\n  LABELS= {{ $labels }}
        - alert: MysqlSlowQueries
          expr: rate(mysql_global_status_slow_queries[5m]) > 0
          for: 2m
          labels:
              severity: warning
          annotations:
            summary: MySQL slow queries (instance {{ $labels.instance }})
            description: MySQL server mysql has some new slow query.\n  VALUE = {{ $value }}\n  LABELS= {{ $labels }}
        - alert: MysqlInnodbLogWaits
          expr: rate(mysql_global_status_innodb_log_waits[15m]) > 10
          for: 0m
          labels:
            severity: warning
          annotations:
            summary: MySQL InnoDB log waits (instance {{ $labels.instance }})
            description: MySQL innodb log writes stalling\n  VALUE = {{ $value }}\n  LABELS= {{ $labels }}
        - alert: Mysql Cache Hit Rate
          expr: rate(mysql_global_status_table_open_cache_hits[5m]) / (rate(mysql_global_status_table_open_cache_hits[5m]) + rate(mysql_global_status_table_open_cache_misses[5m])) < 0.8
          for: 1m
          labels:
            severity: warning
          annotations:
            summary: MySQL Cache Hit Rate is low (instance {{ $labels.instance }})
            description: MySQL Cache Hit Rate is low (instance {{ $labels.instance }})
        - alert: MysqlHighThreadsRunning
          expr: avg by (instance) (mysql_global_status_threads_running) / avg by (instance) (mysql_global_variables_max_connections) * 100 > 60
          for: 2m
          labels:
            severity: warning
          annotations:
            summary: MySQL high threads running (instance {{ $labels.instance }})
            description: More than 60% of MySQL connections are in running state on {{ $labels.instance }}\n  VALUE = {{ $value }}\n  LABELS= {{ $labels }}
        - alert: MysqlSlaveIoThreadNotRunning
          expr: mysql_slave_status_master_server_id > 0 and ON (instance) mysql_slave_status_slave_io_running == 0
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: MySQL Slave IO thread not running (instance {{ $labels.instance }})
            description: MySQL Slave IO thread not running on {{ $labels.instance }}\n  VALUE = {{ $value }}\n  LABELS= {{ $labels }}
        - alert: MysqlRestarted
          expr: mysql_global_status_uptime < 60
          for: 0m
          labels:
            severity: info
          annotations:
            summary: MySQL restarted (instance {{ $labels.instance }})
            description: MySQL has just been restarted, less than one minute ago on {{ $labels.instance }}.\n  VALUE = {{ $value }}\n  LABELS= {{ $labels }}

      - name: mongodb
        rules:
        - alert: MongoServerDown
          expr: up{job="mongodb-metrics"} == 0
          for: 1s
          labels:
            severity: warning
          annotations:
            summary: Mongo server detected down by instance {{$labels.instance}} in {{$labels.namespace}}
        - alert: HighLatency
          expr: rate(mongodb_mongod_op_latencies_latency_total[5m]) / rate(mongodb_mongod_op_latencies_ops_total[5m]) > 35000
          for: 10m
          labels:
            severity: page
          annotations:
            summary: High latency in instance {{$labels.instance}}
        - alert: HighTicketUtilization
          expr: (mongodb_mongod_wiredtiger_concurrent_transactions_out_tickets / mongodb_mongod_wiredtiger_concurrent_transactions_total_tickets) > 0.75
          for: 10m
          labels:
            severity: page
          annotations:
            summary: Ticket usage over 75% in instance {{$labels.instance}}
        - alert: MongodbTooManyConnections
          expr: avg by(instance) (rate(mongodb_ss_connections{conn_type="current"}[1m])) / avg by(instance) (sum (mongodb_ss_connections) by (instance)) * 100 > 80
          for: 2m
          labels:
            severity: warning
          annotations:
            summary: MongoDB too many connections (instance {{ $labels.instance }})
            description: "Too many connections (> 80%)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
        - alert: MongodbReplicationLag
          expr: avg(mongodb_mongod_replset_member_optime_date{state="PRIMARY"}) - avg(mongodb_mongod_replset_member_optime_date{state="SECONDARY"}) > 10
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: MongoDB replication lag (instance {{ $labels.instance }})
            description: "Mongodb replication lag is more than 10s\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
      - name: Redis
        rules:
        - alert: RedisDown
          expr: redis_up == 0
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Redis down (instance {{ $labels.instance }})
            description: "Redis instance is down\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
        - alert: RedisDisconnectedSlaves
          expr: count without (instance, job) (redis_connected_slaves) - sum without (instance, job) (redis_connected_slaves) - 1 > 1
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Redis disconnected slaves (instance {{ $labels.instance }})
            description: "Redis not replicating for all slaves. Consider reviewing the redis replication status.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
        - alert: RedisTooManyConnections
          expr: redis_connected_clients > 1750
          for: 2m
          labels:
            severity: warning
          annotations:
            summary: Redis too many connections (instance {{ $labels.instance }})
            description: "Redis instance has too many connections\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
        - alert: RedisReplicationBroken
          expr: delta(redis_connected_slaves[1m]) < 0
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Redis replication broken (instance {{ $labels.instance }})
            description: "Redis instance lost a slave\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
        - alert: RedisHighResponseTime
          expr: sum(rate(redis_commands_duration_seconds_total[5m])) / sum(rate(redis_commands_processed_total[5m])) > 0.250
          for: 10m
          labels:
            severity: page
          annotations:
            summary: Response time over 250ms in instance {{$labels.instance}}
        - alert: RedisHighKeysEvictionRatio
          expr: (sum(rate(redis_evicted_keys_total[5m])) / sum(redis_db_keys)) > 0.1
          for: 30m
          labels:
            severity: page
          annotations:
            summary: High keys eviction ratio in instance {{$labels.instance}}

      - name: Jenkins Alerts
        rules:
        - alert: JenkinsInstanceUnhealthy
          expr: jenkins_health_check_score < 0.5
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: Jenkins server is unhealthy
            description: "Check jenkins health. Current health values is {{ $value }}"
        - alert: JenkinsFailedJobs
          expr: rate(jenkins_runs_failure_total[1h]) > 1
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: Jenkins Jobs Failing
            description: "Jenkins jobs are failing\n  VALUE = {{ $value }}\n"
      - name: Elastic Search Alerts
        rules:
        - alert: ElasticSearchClusterUnhealthy
          expr: elasticsearch_cluster_health_status{color="red"}==1
          for: 1m
          labels:
            severity: warning
          annotations:
            summary: Elastic Search Cluster Is Unhealthy
            description: "Elastic Search Cluster is in Unhealthy State. Current health values is RED"
        - alert: ElasticSearchHighHeapMemoryUsage
          expr: elasticsearch_jvm_memory_used_bytes{area="heap"} / elasticsearch_jvm_memory_max_bytes{area="heap"} > 0.7
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: ElasticSearch node {{ $labels.name }} heap usage is high
            description: The heap usage in {{ $labels.name }} is over 80% for 10m.
        - alert: ElasticSearchLowHeapMemoryUsage
          expr: elasticsearch_jvm_memory_used_bytes{area="heap"} / elasticsearch_jvm_memory_max_bytes{area="heap"} < 0.15
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: ElasticSearch node {{ $labels.name }} heap usage is high
            description: The heap usage in {{ $labels.name }} is less than 15% for 10m.

      - name: Rabbitmq
        rules:
        - alert: RabbitmqNodeDown
          expr: sum(rabbitmq_build_info) < 2
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Rabbitmq node down (instance {{ $labels.instance }})
            description: "Less than 3 nodes running in RabbitMQ cluster\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
        - alert: RabbitmqTooManyUnackMessages
          expr: sum(rabbitmq_queue_messages_unacked) BY (QUEUE) > 1000
          for: 1m
          labels:
            severity: warning
          annotations:
            summary: Rabbitmq too many unack messages (instance {{ $labels.instance }})
            description: "Too many unacknowledged messages\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
        - alert: RabbitmqTooManyConnections
          expr: rabbitmq_connections > 1000
          for: 2m
          labels:
            severity: warning
          annotations:
            summary: Rabbitmq too many connections (instance {{ $labels.instance }})
            description: "The total connections of a node is too high\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
        - alert: RabbitmqUnroutableMessages
          expr: sum by(namespace, rabbitmq_cluster) (increase(rabbitmq_channel_messages_unroutable_dropped_total[5m]) * on(instance) group_left(rabbitmq_cluster) rabbitmq_identity_info) >= 1
          for: 2m
          labels:
            severity: warning
          annotations:
            summary: Rabbitmq unroutable messages (instance {{ $labels.instance }})
            description: "A queue has unroutable messages\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
        - alert: RabbitmqFileDescriptorsNearLimit
          expr: sum by(namespace, rabbitmq_cluster, pod, rabbitmq_node) (max_over_time(rabbitmq_process_open_fds[5m]) * on(instance) group_left(rabbitmq_cluster, rabbitmq_node, pod) rabbitmq_identity_info) / sum by(namespace, rabbitmq_cluster, pod, rabbitmq_node) (rabbitmq_process_max_tcp_sockets * on(instance) group_left(rabbitmq_cluster, rabbitmq_node, pod) rabbitmq_identity_info) > 0.8
          for: 10m
          annotations:
            description: |
              `{{ $value | humanizePercentage }}` file descriptors of file
              descriptor limit are used in RabbitMQ node `{{ $labels.rabbitmq_node }}`,
              pod `{{ $labels.pod }}`, RabbitMQ cluster `{{ $labels.rabbitmq_cluster }}`,
              namespace `{{ $labels.namespace }}`.
            summary: |
              More than 80% of file descriptors are used on the RabbitMQ node.
              When this value reaches 100%, new connections will not be accepted and disk write operations may fail.
              Client libraries, peer nodes and CLI tools will not be able to connect when the node runs out of available file descriptors.
              See https://www.rabbitmq.com/production-checklist.html#resource-limits-file-handle-limit.
          labels:
            rulesgroup: rabbitmq
            severity: warning


#Thanos Configuration
thanosRuler:
  enabled: true

  ## Annotations for ThanosRuler
  ##
  annotations: {}

  ## Service account for ThanosRuler to use.
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  ##
  serviceAccount:
    create: true
    name: ""
    annotations: {}

  ## Configure pod disruption budgets for ThanosRuler
  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/#specifying-a-poddisruptionbudget
  ##
  podDisruptionBudget:
    enabled: true
    minAvailable: 1
    maxUnavailable: ""

  serviceMonitor:
    ## If true, create a serviceMonitor for thanosRuler
    ##
    selfMonitor: true

    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
    ##
    interval: ""

    ## Additional labels
    ##
    additionalLabels: {}

    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
    ##
    sampleLimit: 0

    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
    ##
    targetLimit: 0

    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    ##
    labelLimit: 0

    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    ##
    labelNameLengthLimit: 0

    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    ##
    labelValueLengthLimit: 0

    ## proxyUrl: URL of a proxy that should be used for scraping.
    ##
    proxyUrl: ""

    ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS.
    scheme: ""

    ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS.
    ## Of type: https://github.com/coreos/prometheus-operator/blob/main/Documentation/api.md#tlsconfig
    tlsConfig: {}

    bearerTokenFile:

    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    ##
    metricRelabelings: []
    # - action: keep
    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
    #   sourceLabels: [__name__]

    ## RelabelConfigs to apply to samples before scraping
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    ##
    relabelings: []
    # - sourceLabels: [__meta_kubernetes_pod_node_name]
    #   separator: ;
    #   regex: ^(.*)$
    #   targetLabel: nodename
    #   replacement: $1
    #   action: replace

    ## Additional Endpoints
    ##
    additionalEndpoints: []
    # - port: oauth-metrics
    #   path: /metrics

  ## Settings affecting thanosRulerpec
  ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#thanosrulerspec
  ##
  thanosRulerSpec:
    ## Standard object's metadata. More info: https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#metadata
    ## Metadata Labels and Annotations gets propagated to the ThanosRuler pods.
    ##
    podMetadata: {}
    ruleNamespaceSelector: {}

    ## If true, a nil or {} value for thanosRuler.thanosRulerSpec.ruleSelector will cause the
    ## prometheus resource to be created with selectors based on values in the helm deployment,
    ## which will also match the PrometheusRule resources created
    ##
    ruleSelectorNilUsesHelmValues: true

    ## PrometheusRules to be selected for target discovery.
    ## If {}, select all PrometheusRules
    ##
    ruleSelector: {}
    ## Example which select all PrometheusRules resources
    ## with label "prometheus" with values any of "example-rules" or "example-rules-2"
    # ruleSelector:
    #   matchExpressions:
    #     - key: prometheus
    #       operator: In
    #       values:
    #         - example-rules
    #         - example-rules-2
    #
    ## Example which select all PrometheusRules resources with label "role" set to "example-rules"
    # ruleSelector:
    #   matchLabels:
    #     role: example-rules

    ## Define Log Format
    # Use logfmt (default) or json logging
    logFormat: logfmt

    ## Log level for ThanosRuler to be configured with.
    ##
    logLevel: info

    ## Size is the expected size of the thanosRuler cluster. The controller will eventually make the size of the
    ## running cluster equal to the expected size.
    replicas: 1

    ## Time duration ThanosRuler shall retain data for. Default is '24h', and must match the regular expression
    ## [0-9]+(ms|s|m|h) (milliseconds seconds minutes hours).
    ##
    retention: 24h

    ## Interval between consecutive evaluations.
    ##
    evaluationInterval: ""

    ## Storage is the definition of how storage will be used by the ThanosRuler instances.
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/storage.md
    ##
    storage: {}
    # volumeClaimTemplate:
    #   spec:
    #     storageClassName: gluster
    #     accessModes: ["ReadWriteOnce"]
    #     resources:
    #       requests:
    #         storage: 50Gi
    #   selector: {}

    ## AlertmanagerConfig define configuration for connecting to alertmanager.
    ## Only available with Thanos v0.10.0 and higher. Maps to the alertmanagers.config Thanos Ruler arg.
    alertmanagersConfig:
      # use existing secret, if configured, alertmanagersConfig.secret will not be used
      existingSecret: {}
        # name: ""
        # key: ""
      # will render render alertmanagersConfig secret data and configure it to be used by Thanos Ruler custom resource, ignored when alertmanagersConfig.existingSecret is set
      # https://thanos.io/tip/components/rule.md/#alertmanager
      secret: {}
        # alertmanagers:
        # - api_version: v2
        #   http_config:
        #     basic_auth:
        #       username: some_user
        #       password: some_pass
        #   static_configs:
        #     - alertmanager.thanos.io
        #   scheme: http
        #   timeout: 10s

    ## DEPRECATED. Define URLs to send alerts to Alertmanager. For Thanos v0.10.0 and higher, alertmanagersConfig should be used instead.
    ## Note: this field will be ignored if alertmanagersConfig is specified. Maps to the alertmanagers.url Thanos Ruler arg.
    # alertmanagersUrl:

    ## The external URL the Thanos Ruler instances will be available under. This is necessary to generate correct URLs. This is necessary if Thanos Ruler is not served from root of a DNS name. string false
    ##
    externalPrefix:

    ## If true, http://{{ template "kube-prometheus-stack.thanosRuler.name" . }}.{{ template "kube-prometheus-stack.namespace" . }}:{{ .Values.thanosRuler.service.port }}
    ## will be used as value for externalPrefix
    externalPrefixNilUsesHelmValues: true

    ## The route prefix ThanosRuler registers HTTP handlers for. This is useful, if using ExternalURL and a proxy is rewriting HTTP routes of a request, and the actual ExternalURL is still true,
    ## but the server serves requests under a different route prefix. For example for use with kubectl proxy.
    ##
    routePrefix: /

    ## ObjectStorageConfig configures object storage in Thanos
    objectStorageConfig:
      # use existing secret, if configured, objectStorageConfig.secret will not be used
        # existingSecret:
        #   name: "thanos-objstore"
        #   key: "objstore.yml"
      # will render objectStorageConfig secret data and configure it to be used by Thanos Ruler custom resource, ignored when objectStorageConfig.existingSecret is set
      # https://thanos.io/tip/thanos/storage.md/#s3
        secret:
          type: S3
          config:
            bucket: ${s3_bucket_name}
            endpoint: ${s3_endpoint}
            region: ${s3_bucket_region}
          #   access_key: ""
          #   secret_key: ""

    ## Labels by name to drop before sending to alertmanager
    ## Maps to the --alert.label-drop flag of thanos ruler.
    alertDropLabels: []

    ## QueryEndpoints defines Thanos querier endpoints from which to query metrics.
    ## Maps to the --query flag of thanos ruler.
    queryEndpoints:  [http://prometheus-prometheus.monitoring.svc.cluster.local:9090/]

    ## Define configuration for connecting to thanos query instances. If this is defined, the queryEndpoints field will be ignored.
    ## Maps to the query.config CLI argument. Only available with thanos v0.11.0 and higher.
    queryConfig:
      # use existing secret, if configured, queryConfig.secret will not be used
      existingSecret: {}
        # name: ""
        # key: ""
      # render queryConfig secret data and configure it to be used by Thanos Ruler custom resource, ignored when queryConfig.existingSecret is set
      # https://thanos.io/tip/components/rule.md/#query-api
      secret: {}
        # - http_config:
        #     basic_auth:
        #       username: some_user
        #       password: some_pass
        #   static_configs:
        #     - URL
        #   scheme: http
        #   timeout: 10s

    ## Labels configure the external label pairs to ThanosRuler. A default replica
    ## label `thanos_ruler_replica` will be always added as a label with the value
    ## of the pod's name and it will be dropped in the alerts.
    labels: {}

    ## If set to true all actions on the underlying managed objects are not going to be performed, except for delete actions.
    ##
    paused: false

    ## Allows setting additional arguments for the ThanosRuler container
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#thanosruler
    ##
    additionalArgs: []
      # - name: remote-write.config
      #   value: |-
      #     "remote_write":
      #     - "name": "receiver-0"
      #       "remote_timeout": "30s"
      #       "url": "http://thanos-receiver-0.thanos-receiver:8081/api/v1/receive"

    ## Define which Nodes the Pods are scheduled on.
    ## ref: https://kubernetes.io/docs/user-guide/node-selection/
    ##
    nodeSelector: {}

    ## Define resources requests and limits for single Pods.
    ## ref: https://kubernetes.io/docs/user-guide/compute-resources/
    ##
    resources:
      requests:
        memory: 200Mi
        cpu: 100m
      limits:
        memory: 400Mi
        cpu: 200m

    ## Pod anti-affinity can prevent the scheduler from placing Prometheus replicas on the same node.
    ## The default value "soft" means that the scheduler should *prefer* to not schedule two replica pods onto the same node but no guarantee is provided.
    ## The value "hard" means that the scheduler is *required* to not schedule two replica pods onto the same node.
    ## The value "" will disable pod anti-affinity so that no anti-affinity rules will be configured.
    ##
    podAntiAffinity: ""

    ## If anti-affinity is enabled sets the topologyKey to use for anti-affinity.
    ## This can be changed to, for example, failure-domain.beta.kubernetes.io/zone
    ##
    podAntiAffinityTopologyKey: kubernetes.io/hostname

    ## Assign custom affinity rules to the thanosRuler instance
    ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
    ##
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: "Addons-Services"
              operator: In
              values:
              - "true"
